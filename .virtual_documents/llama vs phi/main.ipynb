#install necessary modules
get_ipython().run_line_magic("pip", " install python-dotenv --quiet")
get_ipython().run_line_magic("pip", " install llama_index --quiet")
#%pip install llama-index-llms-cohere --quiet
#%pip install llama-index-embeddings-cohere --quiet
#%pip install llama-index-postprocessor-cohere_rerank --quiet
get_ipython().run_line_magic("pip", " install llama-index-llms-ollama --quiet")
get_ipython().run_line_magic("pip", " install llama-index-embeddings-huggingface --quiet")


get_ipython().run_line_magic("pip", " install llama-index-embeddings-huggingface --quiet")


import nest_asyncio
from dotenv import load_dotenv
from IPython.display import Markdown, display
from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.core import PromptTemplate
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex, ServiceContext, SimpleDirectoryReader


# allows nested access to the event loop
nest_asyncio.apply()



# add your documents in this directory, you can drag & drop
input_dir_path = '/home/ec2-user/SageMaker/llama vs phi/test-dir'


# setup llm & embedding model
# llm=Ollama(model="phi3", request_timeout=120.0)
llm=Ollama(model="llama3", request_timeout=120.0)
# embed_model = HuggingFaceEmbedding( model_name="Snowflake/snowflake-arctic-embed-m", trust_remote_code=True)
embed_model = HuggingFaceEmbedding( model_name="BAAI/bge-large-en-v1.5", trust_remote_code=True)


#test
llm=Ollama(model="llama2", request_timeout=30.0)



resp = llm.complete("Who is Paul Graham?")


# load data
loader = SimpleDirectoryReader(
            input_dir = input_dir_path,
            required_exts=[".pdf"],
            recursive=True
        )
docs = loader.load_data()

# Creating an index over loaded data
Settings.embed_model = embed_model
index = VectorStoreIndex.from_documents(docs, show_progress=True)

# Create the query engine, where we use a cohere reranker on the fetched nodes
Settings.llm = llm
query_engine = index.as_query_engine(similarity_top_k=1)

# ====== Customise prompt template ======
qa_prompt_tmpl_str = (
"Context information is below.\n"
"---------------------\n"
"{context_str}\n"
"---------------------\n"
"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\n"
"Query: {query_str}\n"
"Answer: "
)
qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)

query_engine.update_prompts(
    {"response_synthesizer:text_qa_template": qa_prompt_tmpl}
)

# Generate the response
response = query_engine.query("What is this research paper about?",)


display(Markdown(str(response)))






